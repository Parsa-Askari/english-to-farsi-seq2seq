{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3c6369c-abc1-424c-bd75-81f1c7c06d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as eng_tokenize\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abf63d0-695e-4bb0-b874-ff25da281ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,en_data,fa_data,en_index,fa_index):\n",
    "        super(dataset,self).__init__()\n",
    "        for seq in en_data:\n",
    "            for i in range(len(seq)):\n",
    "                seq[i]=en_index[seq[i]]\n",
    "        for seq in fa_data:\n",
    "            for i in range(len(seq)):\n",
    "                seq[i]=fa_index[seq[i]]\n",
    "        self.en_data=torch.from_numpy(pad_sequences(en_data,padding=\"post\",maxlen=10,value=1))\n",
    "        self.fa_data=torch.from_numpy(pad_sequences(fa_data,padding=\"post\",maxlen=10,value=1))\n",
    "    def __len__(self):\n",
    "        return len(self.en_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.en_data[idx],self.fa_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6417f4a-51aa-4656-9d7e-b6575499abf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./hugg_preprocessed/vocab_en\", \"rb\") as fp:   # Unpickling\n",
    "    vocab_en = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/vocab_fa\", \"rb\") as fp:   # Unpickling\n",
    "    vocab_fp = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/dataset_en\", \"rb\") as fp:   # Unpickling\n",
    "    dataset_en = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/dataset_fa\", \"rb\") as fp:   # Unpickling\n",
    "    dataset_fa = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/en_index.json\", \"r\") as fp:   #Pickling\n",
    "    en_index=json.load(fp)\n",
    "with open(\"./hugg_preprocessed/fa_index.json\", \"r\") as fp:   #Pickling\n",
    "    fa_index=json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bd7e45-0ef9-4365-914d-8370e5666106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds=dataset(dataset_en,dataset_fa,en_index,fa_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f1d5ae74-9361-44ed-b731-cbb91ef29953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,hidden_size,embeding_size,vocab_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_net=nn.Embedding(num_embeddings=len(vocab),embedding_dim=embeding_dim)\n",
    "        \n",
    "        self.WF=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size))\n",
    "        self.BF=nn.Parameter(torch.rand(1,hidden_size))\n",
    "        self.sigF=nn.Sigmoid()\n",
    "        \n",
    "        self.WI1=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size))\n",
    "        self.BI1=nn.Parameter(torch.rand(1,hidden_size))\n",
    "        self.sigI=nn.Sigmoid()\n",
    "        self.WI2=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size))\n",
    "        self.BI2=nn.Parameter(torch.rand(1,hidden_size))\n",
    "        self.tanhI=nn.Tanh()\n",
    "        \n",
    "        self.WO=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size))\n",
    "        self.BO=nn.Parameter(torch.rand(1,hidden_size))\n",
    "        self.tanhO=nn.Tanh()\n",
    "        self.sigO=nn.Sigmoid()\n",
    "        \n",
    "        self.output_network=nn.ParameterList([\n",
    "            nn.Linear(in_features=hidden_size+hidden_size,out_features=200),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Linear(in_features=200,out_features=vocab_size),\n",
    "            nn.Softmax(dim=1),\n",
    "            nn.Dropout(0.2)\n",
    "        ])\n",
    "        \n",
    "    def forward(self,x_batch,short_memory,long_memory):\n",
    "        \"\"\"\n",
    "        x_batch = (batch_size,embeding_size)\n",
    "        short_memory =(batch_size,hidden_size)\n",
    "        long_memory =(batch_size,hidden_size)\n",
    "        \"\"\"\n",
    "        emb_batch=self.input_net(x_batch)\n",
    "        scaler=emb_batch.shape[0]\n",
    "        #Forget gate\n",
    "        new_batch=torch.concat((short_memory,emb_batch),dim=1) #(batch_size,hidden_size+embeding_size)\n",
    "        zF=torch.matmul(new_batch,self.WF)/scaler +self.BF #batch_size,hidden_size\n",
    "        aF=self.sigF(zF)\n",
    "        \n",
    "        #Input gate\n",
    "        zI1=torch.matmul(new_batch,self.WI1)/scaler + self.BI1 #batch_size,hidden_size\n",
    "        aI1=self.sigI(zI1)\n",
    "        \n",
    "        zI2=torch.matmul(new_batch,self.WI2)/scaler +self.BI2 #batch_size,hidden_size\n",
    "        aI2=self.sigI(zI2)\n",
    "        aI=aI1*aI2\n",
    "        \n",
    "        #Output gate\n",
    "        long_memory=(long_memory*aF)+(long_memory*aI) #batch_size,hidden_size\n",
    "        \n",
    "        zO1=torch.matmul(new_batch,self.WO)/scaler +self.BO #batch_size,hidden_size\n",
    "        aO1=self.sigO(zO1)\n",
    "        \n",
    "        aO2=self.tanhO(long_memory)\n",
    "        \n",
    "        short_memory=aO1*aO2\n",
    "        \n",
    "        x=torch.concat((short_memory,long_memory),dim=1)\n",
    "        for l in self.output_network:\n",
    "            x=l(x)\n",
    "        return short_memory,long_memory,x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "77fc6e54-094f-4fb3-83df-66212e421f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs=[\"my name is parsa\",\"hi , nice to meet you\",\"i am very sad.\",\"its nice to see you again\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "024df07a-207a-495a-8e51-ba4411e6f1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer=keras.layers.TextVectorization()\n",
    "vectorizer.adapt(docs)\n",
    "vocab=vectorizer.get_vocabulary()\n",
    "dataset=torch.from_numpy(vectorizer(docs).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9ed4c0c0-d0c3-4a8f-9271-28533e175cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_size=256\n",
    "embeding_dim=128\n",
    "batch_size=2\n",
    "layer_number=2\n",
    "vocab_size=len(vocab)\n",
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f33c3f6b-5e8a-4861-8e97-30b034d532de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f08ba325300>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "3d7bae31-459d-476d-ae2d-4a1ba85681dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.883557677268982\n",
      "2.8524888515472413\n",
      "2.8958346128463743\n",
      "2.8897704124450683\n",
      "2.891349744796753\n",
      "2.849331021308899\n",
      "2.908386993408203\n",
      "2.8767438650131227\n",
      "2.8703625917434694\n",
      "2.854630160331726\n",
      "2.8640942335128785\n",
      "2.861625337600708\n",
      "2.866396594047546\n",
      "2.865682768821716\n",
      "2.8760165214538573\n",
      "2.8612011671066284\n",
      "2.8107142448425293\n",
      "2.807585668563843\n",
      "2.7391353607177735\n",
      "2.786743998527527\n",
      "2.794482398033142\n",
      "2.7813695907592773\n",
      "2.814767861366272\n",
      "2.8705655574798583\n",
      "2.652857446670532\n",
      "2.8766693592071535\n",
      "2.640303540229797\n",
      "2.765291619300842\n",
      "2.8045185089111326\n",
      "2.7939653158187867\n",
      "2.797621965408325\n",
      "2.785860705375671\n",
      "2.8570302724838257\n",
      "2.69811532497406\n",
      "2.6587684392929076\n",
      "2.871030831336975\n",
      "2.7634974360466003\n",
      "2.6939253568649293\n",
      "2.842756915092468\n",
      "2.7862711191177367\n",
      "2.715727925300598\n",
      "2.750377523899078\n",
      "2.7981619358062746\n",
      "2.773526334762573\n",
      "2.8170146703720094\n",
      "2.7043723464012146\n",
      "2.8026283264160154\n",
      "2.841095495223999\n",
      "2.679882764816284\n",
      "2.816312074661255\n"
     ]
    }
   ],
   "source": [
    "model=LSTM(hidden_size=hidden_size,embeding_size=embeding_dim,vocab_size=vocab_size)\n",
    "oprtimizer=torch.optim.Adam(params=model.parameters(),lr=0.001)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for ep in range(epochs):\n",
    "    loss_list=[]\n",
    "    short_memory=torch.rand(batch.shape[0],hidden_size)\n",
    "    long_memory=torch.rand(batch.shape[0],hidden_size)\n",
    "    for i in range(0,len(dataset),batch_size):\n",
    "        batch=dataset[i:i+batch_size]\n",
    "        \n",
    "        for i in range(batch.shape[1]-1):\n",
    "            x_batch=batch[:,i]\n",
    "            new_short,new_long,y_pred=model(x_batch,short_memory,long_memory)\n",
    "            loss=loss_fn(input=y_pred,target=batch[:,i+1])\n",
    "            loss.backward()\n",
    "            oprtimizer.step()\n",
    "            oprtimizer.zero_grad()\n",
    "            short_memory=new_short.detach()\n",
    "            long_memory=new_long.detach()\n",
    "            loss_list.append(loss.detach().item())\n",
    "    print(np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347512d-1987-4cca-b13f-7e18cb3c56a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b31da40-5f65-4b0c-9b6e-7b7d31ef76a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fdadd682-23ab-4ff5-a3c9-b7627e5b26dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([258, 256])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.Tensor(hidden_size+2,hidden_size)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e61960af-482d-4ea1-bdf7-f5d22df24538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls=torch.tensor([[1,2,10,4,5],[3,4,5,4,5],[10,7,9,4,5]])\n",
    "emb=nn.Embedding(num_embeddings=11,embedding_dim=4)\n",
    "emb(ls[:,1]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
