{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fce603-c8ad-4b6a-bf7d-b69d9a4e08a4",
   "metadata": {},
   "source": [
    "# wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40eaa3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hazm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hazm import word_tokenize,Normalizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as eng_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "# nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34efd851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "farsi_path=\"./PEPC_Bidirectional/wiki_extracted_200k.fa\"\n",
    "english_path=\"./PEPC_Bidirectional/wiki_extracted_200k.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b756a3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class preprocess1:\n",
    "    def __init__(self):\n",
    "        self.vocab_fa=[]\n",
    "        self.vocab_en=[]\n",
    "        self.dataset_en=[]\n",
    "        self.dataset_fa=[]\n",
    "        self.EOS=\"EOS\"\n",
    "        self.punctuation_pattern = '[' + re.escape('!\"#$%&\\'()*+-./:;<=>@[\\\\]^_`{|}~') + ']'\n",
    "        self.number_pattern = r'\\d'\n",
    "    def ReadCorpus(self,farsi_path,english_path):\n",
    "        with open(farsi_path,\"r\") as f:\n",
    "            self.farsi_corpus=f.readlines()\n",
    "        with open(english_path,\"r\") as f:\n",
    "            self.english_corpus=f.readlines()\n",
    "    def Tokenize(self):\n",
    "        for i in tqdm(range(len(self.english_corpus))):\n",
    "            en_tokenized=eng_tokenize(self.english_corpus[i])\n",
    "            fa_tokenized=word_tokenize(self.farsi_corpus[i])\n",
    "            en_tokenized.append(self.EOS)\n",
    "            fa_tokenized.append(self.EOS)\n",
    "            en_tokenized,fa_tokenized=self.Clean(en_tokenized,fa_tokenized)\n",
    "            self.dataset_en.append(en_tokenized)\n",
    "            self.dataset_fa.append(fa_tokenized)\n",
    "            self.vocab_en+=en_tokenized\n",
    "            self.vocab_fa+=fa_tokenized\n",
    "        self.vocab_en=sorted(list(set(self.vocab_en)))\n",
    "        self.vocab_fa=sorted(list(set(self.vocab_fa)))\n",
    "    def IndexVocab(self):\n",
    "        self.en_index={}\n",
    "        self.fa_index={}\n",
    "        for i,x in enumerate(self.vocab_en):\n",
    "            self.en_index[x]=i\n",
    "        for i,x in enumerate(self.vocab_fa):\n",
    "            \n",
    "            self.fa_index[x]=i\n",
    "    def Clean(self,fa_tokenized,en_tokenized):\n",
    "        en_tokenized=[x for x in en_tokenized if not re.search(self.punctuation_pattern, x) and not re.search(self.number_pattern, x)]\n",
    "        fa_tokenized=[x for x in fa_tokenized if not re.search(self.punctuation_pattern, x) and not re.search(self.number_pattern, x)]\n",
    "        return en_tokenized,fa_tokenized\n",
    "    def Transfrom(self,en_path,fa_path,del_emoji=False,del_numbers=False):\n",
    "        self.del_emoji=del_emoji\n",
    "        self.del_numbers=del_numbers\n",
    "        self.ReadCorpus(fa_path,en_path)\n",
    "        self.Tokenize()\n",
    "        self.IndexVocab()\n",
    "        return self.vocab_en,self.vocab_fa,self.dataset_en,self.dataset_fa,self.en_index,self.fa_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151d8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prep1=preprocess1()\n",
    "vocab_en,vocab_fa,dataset_en,dataset_fa,en_index,fa_index=prep1.Transfrom(en_path=english_path,fa_path=farsi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12197526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./preprocessed/vocab_en\", \"wb\") as fp:   \n",
    "    pickle.dump(vocab_en, fp)\n",
    "with open(\"./preprocessed/vocab_fa\", \"wb\") as fp:  \n",
    "    pickle.dump(vocab_fa, fp)\n",
    "with open(\"./preprocessed/dataset_en\", \"wb\") as fp:   \n",
    "    pickle.dump(dataset_en, fp)\n",
    "with open(\"./preprocessed/dataset_fa\", \"wb\") as fp:\n",
    "    pickle.dump(dataset_fa, fp)\n",
    "with open(\"./preprocessed/en_index.json\", \"w\") as outfile: \n",
    "    json.dump(en_index, outfile)\n",
    "with open(\"./preprocessed/fa_index.json\", \"w\") as outfile: \n",
    "    json.dump(fa_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7c0c5-1f88-4864-8d1d-fd714d8cdaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "inputString=\"۱ dldl\"\n",
    "punctuation_pattern = '[' + re.escape('!\"#$%&\\'()*+-/:;<=>@[\\\\]^_`{|}~') + ']'\n",
    "number_pattern = r'\\d'\n",
    "bool(re.search(number_pattern, inputString))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacbb2f-a304-48d5-83d9-69e311b8db05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ae759-22b9-484c-ac57-dc18fc94cc89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next(iter(imdb_dataset))[\"translation\"]\n",
    "dataset=load_dataset(\"tep_en_fa_para\")[\"train\"][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35330b35-2fe5-438e-b50b-c1283491448b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cbaa6666464efb8f36f73279937654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_en=[]\n",
    "dataset_fa=[]\n",
    "vocab_en=[]\n",
    "vocab_fa=[]\n",
    "normalizer = Normalizer()\n",
    "def clean(farsi_s,eng_s):\n",
    "    farsi_s=re.sub(r\",+\",\" , \",farsi_s)\n",
    "    farsi_s=re.sub(r\".+\",\" . \",farsi_s)\n",
    "    farsi_s=re.sub(r\"_+\",\" _ \",farsi_s)\n",
    "    farsi_s=re.sub(r\"$\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"#\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"=\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"@\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"\\d\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"~\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"\\'\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\">\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"<\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"\\+\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"\\-\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"\\/\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"\\*\",\"\",farsi_s)\n",
    "    farsi_s=re.sub(r\"\\\"\",\"\",farsi_s)\n",
    "    \n",
    "    eng_s=re.sub(r\",+\",\" , \",eng_s)\n",
    "    eng_s=re.sub(r\".+\",\" . \",eng_s)\n",
    "    eng_s=re.sub(r\"_+\",\" _ \",eng_s)\n",
    "    eng_s=re.sub(r\"$\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"#\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"=\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"@\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"\\d\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"~\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"\\'\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\">\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"<\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"\\+\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"\\-\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"\\/\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"\\*\",\"\",eng_s)\n",
    "    eng_s=re.sub(r\"\\\"\",\"\",eng_s)\n",
    "    return farsi_s,eng_s\n",
    "for dic in tqdm(dataset):\n",
    "    farsi_s=dic[\"fa\"]\n",
    "    eng_s=dic[\"fa\"]\n",
    "    farsi_s,eng_s=clean(farsi_s,eng_s)\n",
    "    dataset_en.append(word_tokenize(normalizer.normalize(farsi_s))+[\"_EOS_\"])\n",
    "    dataset_fa.append(word_tokenize(normalizer.normalize(eng_s))+[\"_EOS_\"])\n",
    "    vocab_en+=dataset_en[-1]\n",
    "    vocab_fa+=dataset_fa[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd12cb6e-c340-48d4-9bfa-2923d0a44804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_en=sorted(list(set(vocab_en)))\n",
    "vocab_fa=sorted(list(set(vocab_fa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c420df-bbf8-4c2a-8710-b8187d6750c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_index={}\n",
    "fa_index={}\n",
    "for i,x in enumerate(vocab_en):\n",
    "    en_index[x]=i\n",
    "for i,x in enumerate(vocab_fa):\n",
    "    fa_index[x]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edf153e-5462-47e2-834c-68ac7b4204aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تاب', 'سس']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizer = Normalizer()\n",
    "s=\"تاب,سس\"\n",
    "s=\" \".join(s.split(\",\"))\n",
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd8e5f5-13d1-4794-9670-1ed28388c183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مسممس', 'مسمس']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"مسممس   مسمس\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2038dc2d-5043-417d-b131-2a3fe9dca761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spsp ,slsl ,,, , , , ss,,,'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"spsp -,slsl ,,, , , , ss,,,\"\n",
    "pat=r',+(?=\\s*\\b)'\n",
    "re.sub(\"\\-\",\"\",s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
