{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6369c-abc1-424c-bd75-81f1c7c06d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as eng_tokenize\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf63d0-695e-4bb0-b874-ff25da281ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self,en_data,fa_data,en_index,fa_index):\n",
    "        super(dataset,self).__init__()\n",
    "        for seq in en_data:\n",
    "            for i in range(len(seq)):\n",
    "                seq[i]=en_index[seq[i]]\n",
    "        for seq in fa_data:\n",
    "            for i in range(len(seq)):\n",
    "                seq[i]=fa_index[seq[i]]\n",
    "        self.en_data=torch.from_numpy(pad_sequences(en_data,padding=\"post\",maxlen=10,value=1))\n",
    "        self.fa_data=torch.from_numpy(pad_sequences(fa_data,padding=\"post\",maxlen=10,value=1))\n",
    "    def __len__(self):\n",
    "        return len(self.en_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.en_data[idx],self.fa_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6417f4a-51aa-4656-9d7e-b6575499abf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./hugg_preprocessed/vocab_en\", \"rb\") as fp:   # Unpickling\n",
    "    vocab_en = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/vocab_fa\", \"rb\") as fp:   # Unpickling\n",
    "    vocab_fp = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/dataset_en\", \"rb\") as fp:   # Unpickling\n",
    "    dataset_en = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/dataset_fa\", \"rb\") as fp:   # Unpickling\n",
    "    dataset_fa = pickle.load(fp)\n",
    "with open(\"./hugg_preprocessed/en_index.json\", \"r\") as fp:   #Pickling\n",
    "    en_index=json.load(fp)\n",
    "with open(\"./hugg_preprocessed/fa_index.json\", \"r\") as fp:   #Pickling\n",
    "    fa_index=json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd7e45-0ef9-4365-914d-8370e5666106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds=dataset(dataset_en,dataset_fa,en_index,fa_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1d5ae74-9361-44ed-b731-cbb91ef29953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CELL(nn.Module):\n",
    "    def __init__(self,hidden_size,embeding_size):\n",
    "        super(CELL,self).__init__()\n",
    "        # self.input_net=nn.Embedding(num_embeddings=len(vocab),embedding_dim=embeding_dim)\n",
    "        \n",
    "        self.WF=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size),requires_grad=True)\n",
    "        self.BF=nn.Parameter(torch.rand(1,hidden_size),requires_grad=True)\n",
    "        self.sigF=nn.Sigmoid()\n",
    "        \n",
    "        self.WI1=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size),requires_grad=True)\n",
    "        self.BI1=nn.Parameter(torch.rand(1,hidden_size),requires_grad=True)\n",
    "        self.sigI=nn.Sigmoid()\n",
    "        self.WI2=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size),requires_grad=True)\n",
    "        self.BI2=nn.Parameter(torch.rand(1,hidden_size),requires_grad=True)\n",
    "        self.tanhI=nn.Tanh()\n",
    "        \n",
    "        self.WO=nn.Parameter(torch.rand(hidden_size+embeding_size,hidden_size),requires_grad=True)\n",
    "        self.BO=nn.Parameter(torch.rand(1,hidden_size),requires_grad=True)\n",
    "        self.tanhO=nn.Tanh()\n",
    "        self.sigO=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x_batch,short_memory,long_memory):\n",
    "        \"\"\"\n",
    "        x_batch = (batch_size,embeding_size)\n",
    "        short_memory =(batch_size,hidden_size)\n",
    "        long_memory =(batch_size,hidden_size)\n",
    "        \"\"\"\n",
    "        # emb_batch=self.input_net(x_batch)\n",
    "        emb_batch=x_batch\n",
    "        scaler=emb_batch.shape[0]\n",
    "        #Forget gate\n",
    "        new_batch=torch.concat((short_memory,emb_batch),dim=1) #(batch_size,hidden_size+embeding_size)\n",
    "        zF=torch.matmul(new_batch,self.WF) +self.BF #batch_size,hidden_size\n",
    "        aF=self.sigF(zF)\n",
    "        \n",
    "        #Input gate\n",
    "        zI1=torch.matmul(new_batch,self.WI1) + self.BI1 #batch_size,hidden_size\n",
    "        aI1=self.sigI(zI1)\n",
    "        \n",
    "        zI2=torch.matmul(new_batch,self.WI2) +self.BI2 #batch_size,hidden_size\n",
    "        aI2=self.tanhI(zI2)\n",
    "        aI=aI1*aI2\n",
    "        \n",
    "        #Output gate\n",
    "        long_memory=(long_memory*aF)+(long_memory+aI) #batch_size,hidden_size\n",
    "        \n",
    "        zO1=torch.matmul(new_batch,self.WO) +self.BO #batch_size,hidden_size\n",
    "        aO1=self.sigO(zO1)\n",
    "        \n",
    "        aO2=self.tanhO(long_memory)\n",
    "        \n",
    "        short_memory=aO1*aO2\n",
    "        \n",
    "        return short_memory,long_memory\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,layer_num,hidden_size,embeding_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        layers=[]\n",
    "        for i in range(layer_num):\n",
    "            layers.append(CELL(hidden_size,embeding_size))\n",
    "            layers.append(nn.Linear(in_features=hidden_size,out_features=embeding_size))\n",
    "        self.Pipeline=nn.ParameterList(layers[:-1])\n",
    "    def forward(self,x,memory_cache):\n",
    "        new_memory_cache=[]\n",
    "        for i,l in enumerate(self.Pipeline):\n",
    "            if((i+1)%2!=0):\n",
    "                h,c=memory_cache[i//2][0],memory_cache[i//2][1]\n",
    "                h_new,c_new=self.Pipeline[i](x,h,c)\n",
    "                new_memory_cache.append([h_new,c_new])\n",
    "                x=h\n",
    "            else:\n",
    "                x=self.Pipeline[i](x)\n",
    "        return new_memory_cache,x\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,hidden_size,embeding_size):\n",
    "        super(Model,self).__init__()\n",
    "        self.cell=CELL(hidden_size,embeding_size)\n",
    "        self.output_network=nn.ParameterList([\n",
    "            nn.Linear(in_features=hidden_size,out_features=1),\n",
    "        ])\n",
    "    def forward(self,x_batch,short_memory,long_memory):\n",
    "        short_memory,long_memory=self.cell(x_batch,short_memory,long_memory)\n",
    "        x=long_memory\n",
    "        for l in self.output_network:\n",
    "            x=l(x)\n",
    "        return short_memory ,long_memory ,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77fc6e54-094f-4fb3-83df-66212e421f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my name is parsa', 'hi , nice to meet you', 'i am very sad.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=[\"my name is parsa\",\"hi , nice to meet you\",\"i am very sad.\",\"its nice to see you again\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024df07a-207a-495a-8e51-ba4411e6f1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer=keras.layers.TextVectorization()\n",
    "vectorizer.adapt(docs)\n",
    "vocab=vectorizer.get_vocabulary()\n",
    "dataset=torch.from_numpy(vectorizer(docs).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47428dc9-1b4d-4743-9eb9-adb45cf06505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "price=pd.read_csv(\"./archive/prices.csv\")\n",
    "data=price[price[\"symbol\"]==\"NOC\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4c0c0-d0c3-4a8f-9271-28533e175cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_size=256\n",
    "embeding_dim=1\n",
    "batch_size=32\n",
    "layer_number=2\n",
    "# vocab_size=len(vocab)\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4470636b-633d-4065-b95f-d53d906b365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1=LSTM(1,2,3)\n",
    "lstm2=LSTM(1,2,3)\n",
    "oprtimizer=torch.optim.Adam(params=[\n",
    "    {'params': lstm1.parameters()},\n",
    "    {'params': lstm2.parameters()}\n",
    "],lr=0.0001)\n",
    "# lstm1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72068c21-e1ab-4193-a01b-f2eb8a940700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67359c9-fa46-448b-a52d-dd377fa7b176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,data,label,symbol,Train=True):\n",
    "        self.x=price[price[\"symbol\"]==symbol].reset_index(drop=True)[label].values.reshape((-1,1))\n",
    "        scaler=MinMaxScaler()\n",
    "        self.x=scaler.fit_transform(self.x)\n",
    "        self.x=torch.tensor(self.x)\n",
    "        self.x=self.x.reshape((-1,2))\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self,indx):\n",
    "        return torch.unsqueeze(self.x[indx][0],dim=-1).float(),torch.unsqueeze(self.x[indx][1],dim=-1).float()\n",
    "train_ds=dataset(data,\"open\",\"NOC\")\n",
    "# valid_ds=dataset(valid,\"open\",\"NOC\")\n",
    "train_dataloader=DataLoader(train_ds,batch_size=batch_size,shuffle=False)\n",
    "# valid_dataloader=DataLoader(valid_ds,batch_size=len(valid_ds),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f06f71-735d-4613-aaf6-211edd5382cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c3f6b-5e8a-4861-8e97-30b034d532de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bae31-459d-476d-ae2d-4a1ba85681dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=Model(hidden_size=hidden_size,embeding_size=embeding_dim)\n",
    "oprtimizer=torch.optim.Adam(params=model.parameters(),lr=0.0001)\n",
    "loss_fn=torch.nn.MSELoss()\n",
    "model.train()\n",
    "for ep in range(epochs):\n",
    "    loss_list=[]\n",
    "    \n",
    "    for x,y in train_dataloader:\n",
    "        short_memory=torch.zeros(32,hidden_size)\n",
    "        long_memory=torch.zeros(32,hidden_size)\n",
    "        if(x.shape[0]!=batch_size):\n",
    "            continue\n",
    "        x_batch=x\n",
    "        new_short,new_long,y_pred=model(x_batch,short_memory,long_memory)\n",
    "        # print(new_short)\n",
    "        loss=loss_fn(input=y_pred.reshape(-1),target=y.reshape(-1))\n",
    "        loss.backward()\n",
    "        oprtimizer.step()\n",
    "        oprtimizer.zero_grad()\n",
    "        short_memory=new_short.detach()\n",
    "        long_memory=new_long.detach()\n",
    "        loss_list.append(loss.detach().item())\n",
    "    print(np.mean(loss_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadd682-23ab-4ff5-a3c9-b7627e5b26dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.Parameter(torch.Tensor(hidden_size+2,hidden_size)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61960af-482d-4ea1-bdf7-f5d22df24538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls=torch.tensor([[1,2,10,4,5],[3,4,5,4,5],[10,7,9,4,5]])\n",
    "emb=nn.Embedding(num_embeddings=11,embedding_dim=4)\n",
    "emb(ls[:,1]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
